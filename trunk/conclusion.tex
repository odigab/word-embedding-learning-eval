\section{Conclusions}

We have performed an extensive extrinsic evaluation of five word embedding methods
under fixed experimental conditions, and evaluated their applicability to four sequence tagging tasks: \pos, \chunking, \ner and \mwe identification.
We found that word embedding features reliably outperformed unigram
features, especially with limited training data, but that there was
relatively little difference over Brown clusters, and
no one embedding method was consistently superior across the different tasks and settings.
Word embeddings and Brown clusters were also found to improve
out-of-domain performance and for OOV words.
We expected a performance gap between the fixed and task-updated embeddings, but the observed difference was marginal.
Indeed, we found that updating can result in overfitting.
We also carried out preliminary analysis of the impact of updating on
the vectors, a direction which we intend to pursue further.

% Finally, by using word embeddings as features for MWE identification, we outperformed 
% the state-of-the-art system on that task.
% %We could not find any trend that suggest that a word embedding method is better than other.
% We hope to build on this in future work by learning representations directly over multiword units.\nss{[Has there been any research into this issue?]}


%%% Local Variables: 
%%% mode: latex
%%% TeX-PDF-mode: t 
%%% TeX-master: "WordEmbEvaluation"
%%% End: 
