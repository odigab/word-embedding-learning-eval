\section{Introduction}

Recently, distributed word representations have grown to become a
mainstay of natural language processing (NLP), and been show to have
empirical utility in a myriad of tasks
\cite{Collobert2008,turian2010word,baroni:2014,Andreas:Klein:2014}.  The
underlying idea behind distributed word representations is simple: to
map each word $w$ in our vocabulary $V$ onto a continuous-valued vector
of dimensionality $d \ll |V|$.  Words that are similar
(e.g., with respect to syntax or lexical semantics) will ideally be mapped to
similar regions of the vector space, implicitly supporting both
generalisation across in-vocabulary (IV) items, and countering the
effects of data sparsity for low-frequency and out-of-vocabulary (OOV)
items.

Without some means of automatically deriving the vector representations
without reliance on labelled data, however, word embeddings would have
little practical utility. Fortunately, it has been shown that they can
be ``pre-trained'' from unlabelled text data using various algorithms 
to model the distributional hypothesis (i.e., that
words which occur in similar contexts tend to be semantically
similar). Pre-training methods have been refined considerably in recent
years, and scaled up to increasingly large corpora.

As with other machine learning methods, it is well known that the
quality of the pre-trained word embeddings depends heavily on factors
including parameter optimization, the size of the training data, and the
fit with the target application. For example, \newcite{turian2010word}
showed that the optimal dimensionality for word embeddings is task-specific.  
One factor which has received relatively little attention in
NLP is the effect of ``updating'' the pre-trained word embeddings as
part of the task-specific training, based on self-taught
learning~\cite{raina2007self}.  Updating leads to word
representations that are task-specific, but often at the cost of
over-fitting low-frequency and OOV words.

% The idea of learning word representations for downstream NLP
% applications embraces the idea of . 
% Self-taught learning starts with learning initial data representations
% from a large amount of unlabelled data, which may not be directly
% relevant to target applications. 
% The learned representations are then fed as features into models of
% target applications, and may be fine-tuned during training to adapt to
% the target needs. 
% Learning from a random sample of unlabelled data is distinct from
% semi-supervised learning~\cite{}, which makes an assumption that the
% unlabelled data shares the same distribution as the labelled training
% data. 
% In the following, we will introduce the recent advances of learning word
% representations and apply them for a range of sequence tagging tasks by
% using graph transformers~\cite{}.

%6) How exactly should word representations be incorporated into feature-based sequence tagging models? (May have been answered by the Guo et al. paper.) 
%I believe there is always a better way of doing this, empirical methods as in Guoâ€™s paper or deep sequence neural networks are the ways to go. However, all of them treat the word embeddings trained with different methods in the same way. I conjecture that we should treat them differently based on how they are trained.


In this paper, we perform an extensive evaluation of five word embedding
approaches under fixed experimental conditions, applied to four sequence
labelling tasks: POS-tagging, full-text chunking, named entity
recognition (NER), and multiword expression (MWE) identification. In
this, we explore the following research questions:
\begin{compactenum}[\bf RQ1:]
\item are word embeddings better than baseline approaches of one-hot
  unigram features and Brown clusters?
\item do word embeddings require less training data (i.e.\ generalise
  better) than one-hot unigram features?
\item what is the impact of updating word embeddings in sequence
  labeling tasks, both empirically over the target task and
  geometrically over the vectors?
\item what is the impact of word embeddings (with and without
  updating) on both OOV items (relative to the training data) and
  out-of-domain data?
\item overall, are some word embeddings better than others in a sequence
  labelling context?
\end{compactenum}


%(i) are word embeddings always better than unigram features? 
%(ii) which word embeddings approach is the best? 
%(iii) to what extend they word embeddings approaches are task specific?
%(iv) how reliable are in labeling out-of-domain data and?

% the conextion between dist. semantics models and sequence taggin. 
%static share hypothesis (in vocabulary words, e.g., individual first names are also rare in the treebank, but tend to cluster together in distributional representations);
%embedding structure hypothesis (e.g., group words by definiteness, like each, this, every, few, most, etc.).


%%% Local Variables: 
%%% mode: latex
%%% TeX-PDF-mode: t 
%%% TeX-master: "WordEmbEvaluation"
%%% End: 
