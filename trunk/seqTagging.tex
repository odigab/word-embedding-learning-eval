\section{Sequence Tagging Tasks}
\label{sec:SeqTagging}

{\color{red}Lizhen}

We evaluate different word representations in four different sequence tagging tasks: POS tagging, chunking, NER and MWE identification. 

For each sequence tagging task, we feed learned word representations into a first order linear-chain graph
transformer~\cite{collobert2011natural}, and trained them by using the online learning algorithm
AdaGrad~\cite{duchi2011adaptive}.
For each model taking distributed word representations as word features, we consider two settings: 

\begin{small}
\begin{itemize}
\item[-] Graph transformer \textit{does not} fine tune word representations during training (this is equivalent to a linear-chain CRF);
\item[-] Graph transformer fine tunes the word representations during training.
\end{itemize}
\end{small}

We consider also CRF models with hand-crafted features, which use one-hot representation for each unigram.

We split the task specific corpus into a training set, validation set, and a test set (see Table \ref{datasplit}). If a corpus already provides fixed splits, we reuse them. For POS-tagging and NER, we also evaluated the models with a out-of-domain corpus (English Web-Treebank and MUC-7, respectively), which has similar annotation schema as the respective training corpus.

\begin{table*}
\caption{Datasets splits and feature space for each sequence tagging task.}
\begin{small}
\begin{tabular}{lllp{3.cm}ll}
\hline
			& \textbf{Training set} & \textbf{Validation set} & \textbf{Test set} & \textbf{Feature space} \\ \hline
\textbf{POS-Tagging} & 0-18 WSJ & 19-21 WSJ & 22-24 of WSJ; English Web-Treebank & as in~\cite{collobert2011natural} \\
\textbf{Chunking} & WSJ & 1000 sentences WSJ & CoNLL-2000 & as in~\cite{turian2010word}\\
\textbf{NER} & CoNLL-2003 train set & CoNLL-2003 dev. set & CoNLL-2003 test set; MUC7 & as in~\cite{turian2010word} \\
\textbf{MWE} & 500 documents from & 100 documents from & 123 documents & as in~\cite{mwecorpus}\\
\hline
\end{tabular}
\label{datasplit}
\end{small}
\end{table*}


In order to have fair and reproducible experimental results, we tuned the hyperparameters with random search~\cite{bergstra2012random}. 
We randomly sampled 50 distinct hyperparameter sets with the same random seed for the models that do not update word embeddings, and sampled 100 distinct hyperparameter sets for the models that update word embeddings. 
For each set of hyperparameters, we train a model on its training set and pick up the best one based on its performance on its validation set~\cite{turian2010word}. 
Note that, we also consider word vector size and context window size of distributed word representation, and number of clusters of brown clustering as the hyperparameters.
This is achieved by mapping each possible hyperparameter combination to the word representation files trained with these parameters. 

However, for the models that update word representations, we always found under-performed hyperparameters after trying out all hyperparameter combinations, because they have more hyperparameters than the models that do not update word representations. Then, for each distributed word representations, we reuse all hyperparameters of the models that do not update word representations, only tune the hyperparameters of AdaGrad for the word representation layer. This method requires only 32 additional runs for each model updating embeddings and achieves consistently better results than 100 random draws.

The final evaluation is carried out in a semi-supervised setting. We split the training set into 10 partitions at log scale. That means, the second smallest partition will be twice the size of the smallest partition. We created 10 training sets with incremental size by merging these partitions from the smallest one to the largest one, and each of them on the same designated test sets. 

We adopted the most commonly used F1 measure as the evaluation metric for all tasks except POS tagging, for which we use per-word accuracy. In order to evaluate model performance on out-of-vocabulary (unknown) words, we reported also the accuracy for the words that do not occur in the training set.

In addition, we also set up experiments to verify if CRF/graph transformer requires different feature design for different kinds of pre-trained word embeddings. This is achieved by adding a hidden layer between CRF and distributed word representations. For each context word, the hidden layer computes the element-wise multiplication of its embedding with the embedding of the current word embedding, and the representation of current word stays the same. The results of this approach are not plotted because this method leads only to marginal improvement.

%\subsection{POS tagging} We could choose one of the options. \subsubsection{Option 1} Almost the same setting as~\cite{collobert2011natural}, except adding one more test set.
% \noindent Training set: 0-18 of WSJ.
% \noindent Validation set: 19-21 of WSJ.
% \noindent Test set: 22-24 of WSJ, and English Web Treebank. We report model performances on these two test sets respectively.
% \noindent Feature space: the same set as in~\cite{collobert2011natural}

% \subsection{Chunking} The same setting as~\cite{turian2010word}\\
% \noindent Training set: WSJ train set.
% \noindent Validation set: Randomly sampled 1000 sentences from the train set for development.
% \noindent Test set: CoNLL2000 test set.
% \noindent Feature space: the same set as in~\cite{turian2010word}

% \subsection{MWE Identification} Training set: randomly sampled 500 documents from Nathana��s corpus. 
% \noindent Validation set: randomly sampled 100 documents from Nathana��s corpus.
% \noindent Test set: remaining 123 documents from Nathana��s corpus..
% \noindent Feature space: the same set as in~\cite{mwecorpus}

%\subsection{Named entity recognition} Training set: CoNLL03 train set.
% \noindent Validation set: CoNLL03 development set.
% \noindent Test set: CoNLL03 test set and MUC7. We report model performances on these two test sets respectively.
% \noindent Feature space: the same set as in~\cite{turian2010word}