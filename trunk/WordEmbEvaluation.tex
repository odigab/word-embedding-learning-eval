%
% File acl2013.tex
%
% Contact  navigli@di.uniroma1.it
%%
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{subfig}
\usepackage{pgfplotstable}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{amssymb}


\title{Evaluation of Word Embeddings for Sequence Tagging Tasks}

\author{A Anonymous 
   \\%NICTA / Locked Bag 8001, \\ Canberra ACT 2601, Australia \\
   \\ %The Australian National University\\
   \\ %University of Canberra \\
  \\ % {\tt \small{@nicta.com.au}} \\
\And
  B Anonymous
   \\%NICTA / Locked Bag 8001, \\ Canberra ACT 2601, Australia \\
   \\%The Australian National University\\ \\
   \\ %{\tt \small{@nicta.com.au}} \\
}

\date{2014}

% Max 8 pp.

\begin{document}
\maketitle
\begin{abstract}
Word embeddings algorithms have the ability to learn word representations from unlabelled data.
The resulting representations are used as features in many NLP applications. 
In this paper, we carry on an extrinsic evaluation of five different word embedding methods
under control conditions, and evaluate them in four sequence labelling tasks: POS-tagging, chunking, NER and MWE identification.
We also evaluate the performance of fine-tuned versus non fine-tuned features during training, and show that fine-tuning can result in over-fitting, when the representations learn unsupervisely were already good.
We also found that when using word embeddings as features, only several hundred of training instances are needed to reach a decent performance. 
Surprising, we could not find any leading word embedding method across the different tasks and proposed settings. Nevertheless, we show that word embeddings are always helping to improve the performance of the evaluated tasks. 
\end{abstract}

\newcommand{\gabi}[1]{\textcolor{blue}{#1}}
\newcommand{\tim}[1]{\textcolor{red}{#1}}
\newcommand{\lizhen}[1]{\textcolor{green}{#1}}
\newcommand{\nss}[1]{\textcolor{magenta}{#1}}

\input{intro}
\input{wordRep}
\input{seqTagging}
\input{results}
\input{relwork}
\input{conclusion}

\section*{Acknowledgments}

Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\

%NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.

\bibliographystyle{acl2013}
\bibliography{biblio}

\end{document}

