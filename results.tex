\section{Experimental Results and Discussion}
During the evaluation, we focus in addressing the following questions:

\textbf{(i) Are the evaluated word embedding methods better than unigram features?}
To answer this question, we systematically compared the usefulness of word embedding versus
unigram features for sequence tagging and noted that word embedding methods always outperform 
unigram features (Figures \ref{fig:bestPOS-Chunk}, \ref{fig:bestNER-MWE}). 

\textbf{(ii) How does the size of labelled training data affect the experimental results?}
We observed that embedding methods are especially helping POS-tagging and NER when there are only several hundreds of training instances. 
Therefore, confirming that any of the evaluated word embedding methods should be used when POS-tagging or NER labelled data is limited.
These early improvements are less evident for chunking and MWE, and according to our results, all the methods, including unigram, needed the same amount of training data to reach a decent performance.
%We attribute this to: a) the standard features used in POS-tagging (e.g., context words, stemmed words) are similar to the representations encoded in the word embeddings.
%  b) NER and MWE require more training data to reach a decent performance; c) the performance of NER and MWE heavily dependent on complex features such as gazetteers and lexicons like Wordnet, which are not captured by the feature representations learned from unlabelled data, meanwhile the standard features used in POS-tagging and chunking (e.g., context words, stemmed words) are similar to the representations encoded in the word embeddings.

\textbf{(iii) How well do the word embeddings perform when evaluated with \textit{out-of-domain} test sets?}
We measure the performance for \textit{out-of-domain} of all the tasks, except MWE identification for which there is no other data set available (see Table \ref{datasplit}).
As expected, the performance goes down, more dramatically for NER for which it drops
about 1 point. The best performing embedding method turn to be skip-gram without fine-tuning and the worst method is unigram.

\textbf{(iv) How well do the word embeddings perform for unknown words?}
As already mentioned, we measure the performance for out-of-vocabulary words (OOV)
in two settings: with \textit{in-domain} and \textit{out-of-domain} corpora, for all the tasks, except MWE identification for which there is no other data set available (see Table \ref{datasplit}).
As expected, word embeddings and Brown clustering excel in \textit{out-of-domain} performance.
Word embeddings without fine-tuning enhance even more the performance of OOV 
for the \textit{in-domain} and \textit{out-of-domain} settings (Figure \ref{OOV}) since fine-tuned
word representations become task-specific, hence performing worst for OOV.

%\textbf{(iv) How do the key parameters of each word learning algorithms affect the experimental results?} 

\textbf{(v) How well do different word embeddings perform in all tasks when semi-supervised fine-tuning is not performed?}, and 
\textbf{(vi) and how well do different word embeddings perform in all tasks when semi-supervised fine-tuning is performed?}
%According to our results, there is not a clear trend about fine-tuning or not during training. 
Across all the methods, fine-tuning is helping POS-Tagging and MWE, where the CW method has been found to be the most sensible to tuning, reaching almost 3 points more, when tuning is performed. 
For chunking and NER, the best results are fine-tuned, but the difference across all the methods and updated features versus not-updated ones, is not significant. 
We also found that fine-tuning can correct poorly learned word representations, but can be
overfitted if unsupervisely learned ones are already good. 

Finally, we address the following question: \textbf{(vii) It has been shown that Brown clusters are useful features for MWE identification but, are also word embeddings helping MWE identification?} 
According to our experiments, the word embedding features distilled by fine-tuned CW reached the best results, beating the state-of-the-art performance (see Table \ref{benchmark}).
However, between Brown clusters and fine-tuned CW, learned under the same settings, the difference is not impressive, suggesting that distributional word representations and cluster-based representations captures similar features for the MWE identification task.
Thus, a natural question: would it be better to learn distributional representations for MWE, instead of representations of single words?

\begin{table*}
\caption{Benchmark results vs. our best results}
\begin{center}
\begin{small}
\begin{tabular}{lll}
\hline
\textbf{Task} & \textbf{Benchmark} & \textbf{Us} \\ \hline
POS-Tagging & (Accuracy) 97.24 \cite{Toutanova:2003} & 0.9592 (skip-gram negsam+up) \\ 
Chunking & (F1) 0.9429 \cite{Sha:2003} & 0.9386 (Brown cluster v2000+)\\  
NER & (F1) 0.8931 \cite{Ando:2005} & 0.8686 (skip-gram negsam+noup)\\  
MWE & (F1) 0.6253 \cite{Schneider+:2014} & 0.6546 (cw+up)\\ 
\hline
\label{benchmark}
\end{tabular}
\end{small}
\end{center}
\end{table*}

As a reference, we compared our best results for each task with their corresponding benchmarks (Table\ref{benchmark}). 
For POS-tagging and chunking, we reach a comparable performance to the state-of-the-art methods.
The difference between our NER system and its baseline is most obvious, as we are 0.025 points below them, but the comparison is not fair considering that their algorithm is much complex
(\newcite{Ando:2005} used a $2^{nd}$ order CRF, while we used a $1^{rst}$ order CRF).
%, which use as features the two previous predictions and the previous label combined with the current word).
For the task of MWE identification, our implementation and settings beat the baseline. 
However, in this paper, we do not aim to maximize the absolute performance of the tasks under 
study, but rather to study the impact of word embeddings for sequence tagging tasks under control settings.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% HEATMAPS 

\iffalse
\newpage
\begin{figure}[htb]
 \subfloat[Some figure]{\includegraphics[scale=0.4]{plots/map-pos-color-invert.pdf}}\hfill
 \subfloat[Some other figure]{\includegraphics[width=0.48\textwidth]\includegraphics[scale=0.4]{plots/map-chunk-color-invert}}\\[-2ex]  %%<-- in this line
 \subfloat[Some more]{\includegraphics[scale=0.4]{plots/map-ner-color-invert}}\hfill
 \subfloat[Some less]\includegraphics[scale=0.4]{plots/map-ner-color-invert}}
 \caption{There are example figures}
\end{figure}
\fi


\newpage

\begin{figure*}
\caption{Best results for each method for POS-Tagging and Chunking. The x-axis correspond to the different word embeddings methods and the y-axis to the 10 training partitions at log scale. Green color stand for high performance, while red color stands for low performance. The methods are in chronological order}
\centering
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-pos-color-invert}    	
	\subcaption{POS-Tagging Accuracy}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-chunk-color-invert}
	\subcaption{Chunking F1-Measure}	
\end{subfigure}
%\\[-2ex]  %%<-- in this line
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-ner-color-invert}    	
	\subcaption{NER F1-Measure}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-mwe-color-invert}
	\subcaption{MWEs F1-Measure}		
\end{subfigure}
\label{fig:bestPOS-Chunk}
\end{figure*}


\iffalse
\begin{figure*}
\caption{Best results for each method for POS-Tagging and Chunking, NER and MWE identification. The x-axis correspond to the different word embeddings methods and the y-axis to the 10 training partitions at log scale. Green color stand for high performance, while red color stands for low performance. The methods are display in chronological order.}
\centering
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-pos}    	
	\subcaption{POS-Tagging Accuracy}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-chunk}
	\subcaption{Chunking F1-Measure}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-ner}    	
	\subcaption{NER F1-Measure}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-mwe}
	\subcaption{MWEs F1-Measure}		
\end{subfigure}
\label{fig:bestPOS-Chunk}
\end{figure*}

\begin{figure*}
\caption{Best results for each method for POS-Tagging and Chunking. The x-axis correspond to the different word embeddings methods and the y-axis to the 10 training partitions at log scale. Green color stand for high performance, while red color stands for low performance. The methods are in chronological order}
\centering
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-pos-color}    	
	\subcaption{POS-Tagging Accuracy}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-chunk-color}
	\subcaption{Chunking F1-Measure}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-ner-color}    	
	\subcaption{NER F1-Measure}	
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    \includegraphics[scale=0.4]{plots/map-mwe-color}
	\subcaption{MWEs F1-Measure}		
\end{subfigure}
\label{fig:bestPOS-Chunk}
\end{figure*}
\fi

\iffalse
\begin{figure*}
\caption{Normalized by best and worst values for each task}
\centering
\includegraphics[scale=0.8]{plots/heat-map}    	
\label{fig:heatmap}
\end{figure*}

\begin{figure*}
\caption{Normalized by best and worst values for each task}
\centering
\includegraphics[scale=0.8]{plots/heat-map-color}    	
\label{fig:heatmap}
\end{figure*}
\fi



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% OOV 
\begin{figure*}
\centering
\caption{Out-of-vocabulary-words (OOV) accuracy for \textit{in-domain} and \textit{out-of-domain} test sets}
\label{OOV} 
    	\includegraphics[scale=0.5]{plots/OOV-plots}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Vector fields
% POS
\begin{figure*}[h]
\caption{updated vs. no-updated word representations for POS}
\centering
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/cw_POS.png}    	
	\label{fig:bestpos}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/cbow_POS.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/glove_POS.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/skip_POS.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\end{figure*}

\iffalse
% chunking
\begin{figure*}[h]
\caption{updated vs. no-updated word representations for chunking}
\centering
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/cw_chunking.png}    	
	\label{fig:bestpos}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/cbow_chunking.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/glove_chunking.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/skip_chunking.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\end{figure*}


% NER
\begin{figure*}[h]
\caption{updated vs. no-updated word representations for NER}
\centering
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/cw_NER.png}    	
	\label{fig:bestpos}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/cbow_NER.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/glove_NER.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/vectorField/skip_NER.png}
	\label{fig:bestchunking}
	\subcaption{}	
\end{subfigure}
\end{figure*}
\fi

\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BEST in PLOTS
\begin{figure*}[h]
\caption{Best results for each method for POS-Tagging and Chunking}
\centering
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/bestPOS}    	
	\label{fig:bestpos}
	\subcaption{POS-Tagging results}	
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    \includegraphics[scale=0.3]{plots/bestChunking}
	\label{fig:bestchunking}
	\subcaption{Chunking results}	
\end{subfigure}
\end{figure*}

\begin{figure*}[h]
\caption{Best results for each method for NER and MWE}
\centering
\begin{subfigure}{6cm}
	\centering
    	\includegraphics[scale=0.3]{plots/bestNER}
	\subcaption{NER results}	
	\label{fig:bestner}
\end{subfigure}
\begin{subfigure}{6cm}
	\centering
    	\includegraphics[scale=0.3]{plots/bestMWE}
    \subcaption{MWE results}	
	\label{fig:bestmwe}
\end{subfigure}  	
\end{figure*}  

\begin{figure*}[h]
\caption{Chunking and MWE out-of-vocabulary-words accuracy for \textit{in-domain} test set}
\centering
\begin{subfigure}{7cm}
	\centering
    	\includegraphics[scale=.4]{plots/Chunking-OOV}
    	\subcaption{Chunking accuracy for OOV}
	\label{fig:inner}
\end{subfigure}
\begin{subfigure}{7cm}
	\centering
    	\includegraphics[scale=0.4]{plots/MWE-OOV}
   	\subcaption{MWE accuracy for OOV}
	\label{fig:outner}
\end{subfigure}  	
\end{figure*}	

\fi


\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% OOV 
\begin{figure*}
\caption{Out-of-vocabulary-words (OOV) accuracy for \textit{in-domain} and \textit{out-of-domain} test sets}
\label{OOV} 
  \begin{minipage}[b]{7cm}
    	\includegraphics[scale=0.4]{plots/POS-OOV-IN}
    \subcaption{POS-Tagging accuracy for \textit{in domain} OOV} 
    \label{POS-OOV-IN}	
  \end{minipage} 
  \begin{minipage}[b]{7cm}
    	\includegraphics[scale=0.4]{plots/POS-OOV-OUT}
    \subcaption{POS-Tagging accuracy for \textit{out domain} OOV} 
    \label{POS-OOV-OUT}	
  \end{minipage} 
  \begin{minipage}[b]{7cm}
    	\includegraphics[scale=0.4]{plots/NER-OOV-IN}
    \subcaption{NER accuracy for \textit{out domain} OOV} 
    \label{NER-OOV-IN}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{7cm}
    	\includegraphics[scale=0.4]{plots/NER-OOV-OUT}
    \subcaption{NER accuracy for \textit{out domain} OOV} 
    \label{NER-OOV-OUT}
  \end{minipage} 
  \begin{minipage}[b]{7cm}
    	\includegraphics[scale=0.4]{plots/Chunking-OOV}
    \subcaption{Chunking accuracy for OOV \textit{in domain} OOV} 
    \label{chunk-OOV}
  \end{minipage} 
    \hfill
  \begin{minipage}[b]{7cm}
    	\includegraphics[scale=0.4]{plots/MWE-OOV}
     \label{POS-OOV}
    \subcaption{MWE accuracy for \textit{in domain} OOV} 
  \end{minipage}   
\end{figure*}
\fi

\iffalse
\subsection{Result tables}
The first column of each table contains the number of training sentences.
\subsubsection{Best hyperparameters for All Tasks}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/POS/Accuracy.csv}
\end{adjustbox}
\caption{Accuracy of POS tagging evaluated on WSJ test set}
\label{table:accuracy_pos}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/NER/CONLL_F1Measure.csv}
\end{adjustbox}
\caption{F1 Measure of NER evaluated on CoNLL test set}
\label{table:f1_ner}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/chunking/chunks_F1Measure.csv}
\end{adjustbox}
\caption{F1 Measure of chunking evaluated on CoNLL test set}
\label{table:f1_chunking}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/MWEs/mwe_F1Measure.csv}
\end{adjustbox}
\caption{F1 Measure of MWE identification evaluated on MWE test set}
\label{table:f1_mwe}
\end{table*}

\subsubsection{Out of Vocabulary results for All Tasks}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/POS/WSJ_out-of-vocabulary_Accuracy.csv}
\end{adjustbox}
\caption{Accuracy of POS tagging evaluated on out-of-vocabulary words in WSJ test set}
\label{table:outVocab_pos_accuracy}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/NER/CONLL_out-of-vocabulary_Accuracy.csv}
\end{adjustbox}
\caption{Accuracy of NER evaluated on out-of-vocabulary words in CoNLL test set}
\label{table:outVocab_ner_accuracy}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/chunking/Accuracy.csv}
\end{adjustbox}
\caption{Accuracy of Chunking evaluated on out-of-vocabulary words in CoNLL test set}
\label{table:outVocab_chunking_accuracy}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/MWEs/out-of-vocabulary_Accuracy.csv}
\end{adjustbox}
\caption{Accuracy of MWE identification evaluated on out-of-vocabulary words in MWE test set}
\label{table:outVocab_mwe_accuracy}
\end{table*}

\subsubsection{Out of domain Results for NER and POS}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/POS/EngWebTreebank_Accuracy.csv}
\end{adjustbox}
\caption{Accuracy of POS tagging evaluated on English web treebank.}
\label{table:outDomain_accuracy_pos}
\end{table*}

\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth}
\pgfplotstabletypeset[
col sep=comma, 
precision=4,
every head row/.style={
before row=\toprule,
after row=\midrule},
every last row/.style={
after row=\bottomrule
}]{eval_results/key_results/NER/MUC7_chunks_F1Measure.csv}
\end{adjustbox}
\caption{F1 measure of NER evaluated on MUC7 test set}
\label{table:outDomain_ner_f1}
\end{table*}

\fi

