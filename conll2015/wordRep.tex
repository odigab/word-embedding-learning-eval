\section{Word Representations}
\label{wordrep} 

\subsection{Types of Word Representations}

\newcite{turian2010word} identifies three varieties of word
representations: \textit{distributional}, \textit{cluster-based}, and
\textit{distributed}.

\textit{Distributional representation} methods map each word $w$ to a
context word vector $\mathbf{C}_w$, which is constructed directly from
co-occurrence counts between $w$ and its context words.  The learning
methods either store the co-occurrence counts between two words $w$ and
$i$ directly in
$C_{wi}$~\cite{sahlgren2006word,turney2010frequency,honkela1997self} or
project the concurrence counts between words into a lower dimensional
space~\cite{vrehuuvrek2010software,lund1996producing}, using
dimensionality reduction techniques such as SVD~\cite{dumais1988using} or
LDA~\cite{blei2003latent}.

\textit{Cluster-based representation} methods build clusters of words by applying either soft or hard clustering algorithms~\cite{lin2009phrase,li2005semi}. Some of them also rely on a co-occurrence matrix of words~\cite{pereira1993distributional}. The Brown clustering algorithm~\cite{Brown92class-basedn-gram} is the best-known method in this category.

\textit{Distributed representation} methods usually map words into dense,
low-dimensional, continuous-valued vectors, with $\mathbf{x} \in
\mathbb{R}^d$, where $d$ is referred to as the word dimension.

\subsection{Selected Word Representations}

Over a range of sequence labelling tasks, we evaluate four methods for
inducing word representations: Brown clustering
\cite{Brown92class-basedn-gram} (``\brown''), the
continuous bag-of-words model (``\CBOW'')~\cite{Mikolov13}, the continuous
skip-gram model (``\Skipgram'')~\cite{Mikolov13NIPS}, and Global vectors
(``\Glove'')~\cite{pennington2014glove}. All have
have been shown to be at or near state-of-the-art in recent empirical
studies~\cite{turian2010word,pennington2014glove}.\footnote{
The word embedding approach proposed in~\newcite{collobert2011natural} is not considered because
it was found to be inferior to our four target word embedding approaches in previous work.}
The training of these word
representations is unsupervised: the common underlying idea is to
predict the occurrence of words in the neighbouring context. Their training
objectives share the same form, which is a sum of local training factors
$J(w, \ctx(w))$,  
\begin{displaymath}
  L = \sum_{w \in T} J(w, \ctx(w))
\end{displaymath}
where $T$ is the set of tokens in a given corpus, and $\ctx(w)$ denotes the
local context of word $w$.
The local context of a word is conventionally its preceding $m$ words,
or alternatively the
$m$ words surrounding it. 
Local training factors are designed to capture the relationship between
$w$ and its local contexts of use, either by predicting $w$
based on its local context, or using $w$ to predict the
context words. Other than \brown, which utilises a cluster-based
representation, all the other methods employ a distributed representation.

The starting point for \CBOW and \Skipgram is to employ softmax to predict word occurrence:
\begin{displaymath}
  J(w, \ctx(w)) = - \log \left( \frac{\exp(\mathbf{v}_w^{\text{T}} \mathbf{v}_{\ctx(w)})}{ \sum_{j \in V} \exp(\mathbf{v}_j^{\text{T}} \mathbf{v}_{\ctx(w)})} \right)
\end{displaymath}
where $\mathbf{v}_{\ctx(w)}$ denotes the distributed representation of
the local context of word $w$, and $V$ is the vocabulary of a given corpus. \CBOW derives $\mathbf{v}_{\ctx(w)}$
based on averaging over the context words. That is, it estimates the
probability of each $w$ given its local
context. In contrast, \Skipgram applies softmax to each context word of
a given occurrence of word $w$. In this case, $\mathbf{v}_{\ctx(w)}$ corresponds to the
representation of one of its context words. This model can be characterised as
predicting context words based on $w$. In practice, softmax is
too expensive to compute over large corpora, and thus~\newcite{Mikolov13NIPS} use
hierarchical softmax and negative sampling to scale up the training.

%\CW considers the local context of a word $w$ to be $m$ words to the left and $m$ words to the right of $w$. The concatenation of the embeddings of $w$ and all its context words are taken as input to a neural network with one hidden layer, which produces a higher level representation $f(w) \in R^d$. Then the learning procedure replaces the embedding of $w$ with that of a randomly sampled word $w'$ and generates a second representation $f(w') \in R^d$ with the same neural network. The training objective is to maximise the difference between them:
%\begin{displaymath}
%J(w, \ctx(w)) = \max (0, 1 - f(w) + f(w'))
%\end{displaymath}
%This approach can be regarded as negative sampling with only one negative example.

\Glove assumes the dot product of two word embeddings should be similar
to the logarithm of the co-occurrence count $X_{ij}$ of the two
words. As such, the local factor $J(w, \ctx(w))$ becomes:
\begin{displaymath}
g(X_{ij}) (\mathbf{v}_i^{\text{T}} \mathbf{v}_j + b_i + b_j - \log(X_{ij}))^2
\end{displaymath}
where $b_i$ and $b_j$ are the bias terms of words $i$ and $j$,
respectively, and $g(X_{ij})$ is a weighting function based on the
co-occurrence count. This weighting function controls the degree of
agreement between the parametric function $\mathbf{v}_i^{\text{T}}
\mathbf{v}_j + b_i + b_j $ and $\log(X_{ij})$. Frequently co-occurring
word pairs will have larger weight
%\nss{i think you mean the weight  will be larger? or there will be more parameters?} 
than infrequent
pairs, up to a threshold.

\brown partitions words into a finite set of word classes $V$. The
conditional probability of seeing the next word is defined to be:
\begin{displaymath}
p(w_k | w_{k - m}^{k -1}) = p(w_k | h_k) p(h_k | h_{k - m}^{k -1})
\end{displaymath}
where $h_k$ denotes the word class of the word $w_k$, $w_{k - m}^{k -1}$
are the previous $m$ words, and $h_{k - m}^{k -1}$ are their respective
word classes. Then $J(w, \ctx(w)) = - \log p(w_k | w_{k - m}^{k
  -1}) $. Since there is no tractable method to find an optimal
partition of word classes, the method uses only a bigram class model, and utilises hierarchical clustering as an approximation method to find a sufficiently good partition of words. 



\subsection{Building Word Representations}
\label{buildingWordRep}

To ensure the comparison of different word representations is fair, we train \brown, \CBOW, \Skipgram, and \Glove on
a fixed corpus, comprised of freely available corpora, as detailed in
\tabref{wordEmbedCorpora}. The joint corpus was preprocessed with the
Stanford CoreNLP sentence splitter and tokeniser. All consecutive digit
substrings were replaced by NUM\textit{f}, where \textit{f} is the
length of the digit substring (e.g., \lex{10.20} is replaced by
\lex{NUM2.NUM2}.
% Due to the computational complexity of the pre-training, for \CW, we simply downloaded the pre-compiled embeddings from:  \url{http://metaoptimize.com/projects/wordreprs}.

\begin{table}[t]
\centering
\smaller
\begin{tabular}{lrc}
\hline
\textbf{Data set} & \multicolumn{1}{c}{\textbf{Size}} & \multicolumn{1}{c}{\textbf{Words}} \\ \hline
UMBC \cite{UMBC:2013}	& 48.1GB & 3G \\
One Billion \cite{OneBillion:2013} & 4.1GB & 1G  \\
English Wikipedia & 49.6GB & 3G \\ \hline
\end{tabular}
\caption{Corpora used to pre-train the word embeddings}
\label{wordEmbedCorpora}
\end{table}

The dimensionality of the word embeddings and the size of the context
window are the key hyperparameters when learning distributed
representations. We use all combinations of the following values to
train word embeddings on the combined corpus:
\begin{itemize}
\item \textbf{Embedding dim.\ $d \in \{25, 50, 100, 200\}$}
\item \textbf{Context window size $m \in \{1, 5, 10\}$}
\end{itemize}

\brown requires only the number of clusters as a hyperparameter. Here, we
perform clustering with $b \in \{250, 500, 1000, 2000, 4000\}$ clusters.
%
%
%Based on the idea that a word is characterized by the company it keeps \cite{firth1957}, 
%distributed word representation methods represent a given word as a continuous vector, which consists of the most frequent contexts of that given word in a big corpus.
%Therefore, similar words have a similar vector representation.
%
%Traditionally, the estimation of the vectors is done by initializing the vectors with co-occurrences counts. More recently, the vectors are learned in a supervised way, where
%the weights in the vectors are set to maximize the probability of the context in which the word is observed in the corpus. 
%Note that the method does no require an annotated corpus since the contexts windows used for training are extracted from unannotated data.
%
%More formally, learning a word vector $W: words \rightarrow \mathbb{R}^n$ is parametrized function that maps
%words to high-dimensional vectors, where $W$ is initialized to have random vectors for each word and learn to have meaningful vectors to perform same task.
%
%Learning the word vectors can be carry out with different models architectures. 
%In this paper, we evaluated five different word embeddings learning algorithms, which are the following: 
% 
%\begin{itemize}
%\item[-] Brown cluster \cite{Brown92class-basedn-gram}
%\item[-] CBOW \cite{Mikolov13NIPS}
%\item[-] Glove \cite{pennington2014glove}
%\item[-] Neural language model \cite{collobert2011natural}
%\item[-] Skip-Gram \cite{Mikolov13}
%\end{itemize}
%
%The above methods were chosen because they are recent
%state-of-the-art word embedding learning methods and because their software is
%available and all of them are neural networks architectures.
%The first method (Brown clusters) was selected as a benchmark word representation, which makes use of hard word clusters rather than a distributed representation.
%
%Skip-Gram \cite{Mikolov13} is a neuronal network language model, but it does not have a hidden layer, and 
%instead predicting the target word, it predicts the context given the target word.
%These embeddings are faster to train than other neuronal embeddings.
%(and does not involve dense matrix multiplication).
%
%The CBOW \cite{Mikolov13NIPS} model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window.
%
%GloVe \cite{pennington2014glove} is essentially a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning.
%
%%One of the benefits of neuronal networks are able to learn ways of representing the data automatically,



%%% Local Variables: 
%%% mode: latex
%%% TeX-PDF-mode: t 
%%% TeX-master: "WordEmbEvaluation"
%%% End: 
