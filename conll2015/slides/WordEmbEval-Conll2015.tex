% Copyright National ICT Australia 2005-2012
%	Author : Anthony Mak
%                Olivier Mehani
%		 William Uther
%
% This program can be redistributed and/or modified under the terms
% of the LaTeX Project Public License Distributed from CTAN
% archives in directory macros/latex/base/lppl.txt.

% If this template does not work, please check you have installed these.
% Required latex packages :-
%		Beamer class version 3.00 or higher
%		pgf version 0.63 or higher
%		xcolor version 2.00 or higher
%		(For details please read Beamer User Guide section 2.1 )
% Required NICTA Beamer Template files :-
%		(Make sure to include the following files in your TeX path)
%			beamerfontthemeNICTA.sty
%			beamercolorthemeNICTA.sty
%			beamercolorthemeNICTAsidebar.sty
%			beamerinnerthemeNICTA.sty
%			beamerouterthemeNICTA.sty
%			beamerthemeNICTA.sty

% /*** NO NEED TO MODIFY ***
\documentclass{beamer}
%\usetheme{NICTA}
%\usetheme[pagenum=false]{NICTA}
\usetheme[sidebar=false]{NICTA}

\usepackage[australian]{babel}
\graphicspath{{graphics/}}
% *** NO NEED TO MODIFY ***/
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfig}
%\usepackage{subcaption}
%\usepackage{pgfplotstable}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[font={small}]{caption}
\usepackage{xspace}
\usepackage{relsize}
\usepackage{authordate1-4}
\usepackage{pifont}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setlength\titlebox{6.5cm}

\newcommand{\RQ}[1][1]{\textbf{RQ#1}\xspace}

\newcommand{\figref}[2][]{Figure~\ref{#2}\xspace}
\newcommand{\tabref}[2][]{Table~\ref{#2}\xspace}

\newcommand{\lex}[1]{\textit{#1}\xspace}

\newcommand{\dataset}[1]{\texttt{#1}\xspace}
\newcommand{\EWT}{\dataset{EWT}}
\newcommand{\WSJ}{\dataset{WSJ}}
\newcommand{\Brown}{\dataset{Brown}}
\newcommand{\Reuters}{\dataset{Reuters}}
\newcommand{\MUC}{\dataset{MUC7}}

\newcommand{\method}[2][]{\ensuremath{\textsc{#2#1}}\xspace}
\newcommand{\unigram}[1][]{\method{Unigram}}
\newcommand{\brown}[1][]{\method[\ensuremath{_{#1}}]{Brown}}
\newcommand{\CW}[1][]{\method[#1]{CW}}
\newcommand{\CBOW}[1][]{\method[#1]{CBOW}}
\newcommand{\Skipgram}[1][]{\method[#1]{Skip-gram}}
\newcommand{\Glove}[1][]{\method[#1]{Glove}}
\newcommand{\withup}{\method{+UP}}

\newcommand{\task}[1]{\textsf{#1}\xspace}
\newcommand{\pos}{\task{POS tagging}}
\newcommand{\chunking}{\task{Chunking}}
\newcommand{\ner}{\task{NER}}
\newcommand{\mwe}{\task{MWE}}

\newcommand{\evmeasure}[1]{\textsc{#1}\xspace}
\newcommand{\accuracy}{\evmeasure{Acc}}
\newcommand{\fscore}{\evmeasure{F1}}

\newcommand{\best}[1]{\textbf{#1}}

\newcommand{\ctx}{\ensuremath{\text{ctx}}}

\hyphenation{an-aly-sis}
\hyphenation{an-aly-ses}
\hyphenation{an-aly-ser}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% TITLE SLIDE %%%%%

\title[Word Representations on Sequence Labelling Tasks] % (optional, use only with long paper titles)
{Big Data Small Data, In Domain Out-of Domain, Known Word Unknown
  Word: The Impact of Word Representations on Sequence Labelling Tasks}

\author[Lizhen Qu et al.]{\textbf{Lizhen Qu}, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, \\
Nathan Schneider \& Timothy Baldwin }


\begin{document}

{\setbeamertemplate{footline}{}
\begin{frame}
  \titlepage
\end{frame}
}





%%%%% FIRST SLIDE %%%%%
\section{Intro}
\label{sec:intro}



\begin{frame}
\frametitle{\textbf{Introduction}}
  \framesubtitle{The Impact of Word Representations for Sequence Labellings Tasks}

\vspace{-1.5cm}
Distributed word representations, pre-trained from unlabelled data 
have become a mainstay of NLP\ldots but

\vspace{0.5cm}
\textbf{What we know about word embeddings?}
\vspace{0.5cm}

\begin{itemize}

\item[\ding{224}] Encode syntactic and semantic properties
\item[\ding{224}] Parameter optimisation is task specific
	\begin{itemize}
	\item[\ding{51}] vector dimension % or word embedding dimention
	\item[\ding{51}] context window size
	\item[\ding{51}] size of the training data
	\end{itemize}
\end{itemize}  
\end{frame}



\begin{frame}
\frametitle{\textbf{What we want to know about word embeddings}}

\begin{itemize}
\item[\bf RQ1:] What is the impact of updating word embeddings in sequence
  labelling tasks
  %, both empirically over the target task and geometrically over the vectors?
\item[]
\item[\bf RQ2:] 
%Do word embeddings require less training data than one-hot unigram features? If so, 
To what degree can word embeddings reduce the amount of labelled data?
\item[]
\item[\bf RQ3:] What is the impact of word embeddings on \textit{out-of-vocabulary} and \textit{out-of-domain data}?
\item[]
%(with and without updating) on both OOV items (relative to the training data) and  out-of-domain data?
\item[\bf RQ4:] Are some word embeddings better than others in a sequence labelling context?
\item[]
%\item[\bf RQ5:] Are word embeddings better than baseline approaches of one-hot unigram features and Brown clusters?
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{\textbf{Extrinsic Evaluation}}
\framesubtitle{Fix the experiments conditions}
  
\begin{itemize}
\item[\ding{224}] \textbf{Selected Word Representations} 
	\begin{itemize}
		\item[\ding{76}] Brown clustering \cite{Brown92class-basedn-gram}
		\item[\ding{76}] CBOW \cite{Mikolov13},
		\item[\ding{76}]	 Skip-gram \cite{Mikolov13NIPS},
		\item[\ding{76}]	 Glove vectors \cite{pennington2014glove} 
	\end{itemize} 
\item[]
\item[\ding{224}] \textbf{In-house pre-trained word embeddings}
	\begin{itemize}			
		\item[\ding{111}] Corpora:
			\begin{itemize}			
				\item[-] UMBC \cite{UMBC:2013}, 48.1GB 
				\item[-] One Billion \cite{OneBillion:2013}, 4.1GB 
				\item[-] English Wikipedia, 49.6GB 		
			\end{itemize}
		\item[\ding{111}] Embedding dimensions:
			\begin{itemize}
				 \item[] $d \in \{25, 50, 100, 200\}$
			\end{itemize}
		\item[\ding{111}] Context window size:
			\begin{itemize}
				 \item[] $m \in \{1, 5, 10\}$	
			\end{itemize}
	\end{itemize} 					
\end{itemize}      
\end{frame}





\begin{frame}
\frametitle{\textbf{Experiments Setup}} 

Four labelling tasks under \textbf{FIX} experiment conditions:

\begin{itemize}
\item[\ding{51}] \pos 
\item[\ding{51}] \chunking
\item[\ding{51}] \ner 
\item[\ding{51}] \mwe (multi-word expressions identification)
\end{itemize}  

\begin{table}
%\centering
\begin{tiny}
\begin{tabular}{@{}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{}}
\hline
& \textbf{Training} & \textbf{Development} & \textbf{\textit{In-domain} Test} & \textbf{\textit{Out-of-domain} Test} \\ \hline
\textbf{\pos} & \WSJ Sec.\ 0-18  & \WSJ Sec.\ 19--21 & \WSJ Sec.\ 22--24 & \EWT  \\
\textbf{\chunking} & \WSJ & \WSJ (1K sentences) & \WSJ (CoNLL-00 test) & \Brown \\
\textbf{\ner} & \Reuters (CoNLL-03 train) & \Reuters (CoNLL-03 dev) & \Reuters (CoNLL-03 test) & \MUC  \\
\textbf{\mwe} & \EWT (500 docs) & \EWT (100 docs)  & \EWT (123 docs) & --- \\
\hline
\end{tabular}
\label{}
\end{tiny}
\end{table}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% HEATMAPS 
\begin{frame}[plain]
%\frametitle{Results} 


\vspace{0.7cm}
\begin{enumerate}
\begin{small}
\item[\bf RQ2:] To what degree can word embeddings reduce the amount of labelled data?
% If so, to what degree can word embeddings reduce the amount of labelled data?
\item[\bf RQ5:] Word embeddings vs. one-hot unigram features vs. Brown clusters?
\end{small}
\end{enumerate}

\vspace{-0.7cm}

\begin{figure}
    \includegraphics[scale=0.48]{../plots/map-ner-color-invert}    	
	\caption{\ner (\fscore)}	
\end{figure}

\end{frame}


\begin{frame}[plain]
%\frametitle{Results} 
%\vspace{-0.7cm}

\begin{columns}
  \column{.55\textwidth}
  \begin{figure}
    \includegraphics[scale=0.22]{../plots/map-pos-color-invert}    	   
    \caption{\pos (\accuracy)}	
	\vspace{-0.3cm}
    \includegraphics[scale=0.22]{../plots/map-chunk-color-invert}
   	\caption{\chunking (\fscore)}		
  \end{figure}  
  \column{.55\textwidth}
    \begin{figure}
    \includegraphics[scale=0.22]{../plots/map-ner-color-invert}    	
	\caption{\ner (\fscore)}	
	\vspace{-0.3cm}
    \includegraphics[scale=0.22]{../plots/map-mwe-color-invert}
	\caption{\mwe (\fscore)}	
     \end{figure}
\end{columns}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Vector fields
\begin{frame}
%\frametitle{Results}

\begin{enumerate}
\begin{small}
\item[\bf RQ3:]: What is the impact of updating word embeddings geometrically over the vectors?
\end{small}
\end{enumerate}

\begin{columns}	
	\column{.55\textwidth}
	  \begin{figure}
	   \includegraphics[width=\textwidth]{../plots/vectorField/Lizhen/scaled/Lizhen_skip_chunking}
	   \caption{\chunking with \Skipgram}
	   \end{figure}
  	\column{.55\textwidth}
	  \begin{figure}
	   \includegraphics[width=\textwidth]{../plots/vectorField/Lizhen/Lizhen_skip_NER}    
   	    \caption{\ner with \Skipgram}
	   \end{figure}	    	
\end{columns}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Plots
\begin{frame}[plain]
\frametitle{}

\vspace{0.7cm}

\begin{itemize}
\begin{small}
\item[\bf RQ4:] What is the impact of these word embeddings (with and without
  updating) on both OOV items (relative to the training data) and
  out-of-domain data?
\item[\bf RQ5:] Are some word embeddings better than others in a sequence labelling context?
\end{small}
\end{itemize}

\includegraphics[scale=0.3]{eval1}
\end{frame}


\begin{frame}
\frametitle{}

\vspace{-0.7cm}

\begin{itemize}
\begin{small}
\item[\bf RQ4:] What is the impact of these word embeddings on both OOV items and out-of-domain data?
\item[\bf RQ5:] Are some word embeddings better than others in a sequence labelling context?
\end{small}
\end{itemize}

\includegraphics[scale=0.3]{eval2}
\end{frame}



\begin{frame}
\frametitle{\textbf{Conclusions}}

% We have performed an extensive extrinsic evaluation on four different word embedding methods under fized conditions, and evaluated their applicability to 4 sequence labelling task.
\begin{itemize}
\item[\ding{51}] In domain: little difference between updating and not-updating word embeddings.
% and updating can result in overfitting
\item[]
\item[\ding{51}] Both word embeddings and Brown clusters capture distributional similarity.
\item[] 
\item[\ding{51}] Both word embeddings and Brown clusters improve cross-domain performance.
\item[]
\item[\ding{51}] Word embeddings and Brown clusters perform well on small amounts of
training data.
\item[]
\item[\ding{51}] No significant differences among word embeddings and Brown clusters.
%Word embeddings outperformed unigram, especially with limited training data, but there is little difference with Brown clusters
%\item[]
%\item[\ding{51}] No one of the word embeddings methods is consistently superior across the different tasks and settings
%\item[]
%\item[\ding{51}] Word embeddings and Brown clusters improve out-of-domain and OOV words
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Last slide
\begin{frame}
\frametitle{}

\begin{itemize}
\centering
%\item[] \uppercase{\textbf{on demand!}}
\item[] Deep Learning Tool Kit for Information Extraction 
\includegraphics[scale=0.3]{dl}
\item[] Pre-trained word embeddings 
\item[\ding{224}]  \url{lizhen.qu@nicta.com.au}
\item[]
\item[] Additional supplementary material 
%(plots and tables)
\item[\ding{224}] \url{https://goo.gl/Y8bk2w}
\item[]
\end{itemize}

\centering \textbf{{\LARGE Thanks!}}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%
%%% Add bibliography 
%%%  splitting the references up into several slides, use the [allowframebreaks]
\begin{frame}[allowframebreaks]
\frametitle{\textbf{References}}
	\bibliographystyle{acl2013}
	\bibliography{../biblio}
\end{frame}


\end{document}

