
\section{Self-taught Learning}
{\color{red}Lizhen and Gabriela}

The idea behind self-taught learning is to use unlabelled data for improving the performance on supervised learning tasks by learning a higher-order feature representation
of the inputs using the unlabelled data \cite{â€¢}

% It is well know that having a good representation of the data  is essential for machine learning algorithms, the key of self-taught learning is to learn that representation from unlabelled data



\subsection{Learning Word Representations}
Based on the idea that a word is characterized by the company it keeps \cite{firth1957}, 
distributed word representation methods represent a given word as a continuous vector, which consists of the most frequent contexts of that given word in a big corpus.
Therefore, similar words have a similar vector representation.

Traditionally, the estimation of the vectors is done by initializing the vectors with co-occurrences counts. More recently, the vectors are learned in a supervised way, where
the weights in the vectors are set to maximize the probability of the context in which the word is observed in the corpus. 
Note that the method does no require an annotated corpus since the contexts windows used for training are extracted from unannotated data.

More formally, learning a word vector $W: words \rightarrow \mathbb{R}^n$ is parametrized function that maps
words to high-dimensional vectors, where $W$ is initialized to have random vectors for each word and learn to have meaningful vectors to perform same task.

Learning the word vectors can be carry out with different models architectures. 
In this paper, we evaluated five different word embeddings learning algorithms, which are the following: 
 
\begin{itemize}
\item[-] Brown cluster \cite{Brown92class-basedn-gram}
\item[-] CBOW \cite{Mikolov13NIPS}
\item[-] Glove \cite{pennington2014glove}
\item[-] Neural language model \cite{collobert2011natural}
\item[-] Skip-Gram \cite{Mikolov13}
\end{itemize}

The above methods were chosen because they are recent
state-of-the-art word embedding learning methods and because their software is
available and all of them are neural networks architectures.
The first method (Brown clusters) was selected as a benchmark word representation, which makes use of hard word clusters rather than a distributed representation.

Skip-Gram \cite{Mikolov13} is a neuronal network language model, but it does not have a hidden layer, and 
instead predicting the target word, it predicts the context given the target word.
These embeddings are faster to train than other neuronal embeddings.
(and does not involve dense matrix multiplication).

The CBOW \cite{Mikolov13NIPS} model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window.

GloVe \cite{pennington2014glove} is essentially a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning.

%One of the benefits of neuronal networks are able to learn ways of representing the data automatically,



