%
% File acl2013.tex
%
% Contact  navigli@di.uniroma1.it
%%
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}



\title{Evaluation of Word Embeddings for Sequence Labelling Tasks}

\author{A Anonymous 
   \\%NICTA / Locked Bag 8001, \\ Canberra ACT 2601, Australia \\
   \\ %The Australian National University\\
   \\ %University of Canberra \\
  \\ % {\tt \small{hanna.suominen@nicta.com.au}} \\
\And
  B Anonymous
   \\%NICTA / Locked Bag 8001, \\ Canberra ACT 2601, Australia \\
   \\%The Australian National University\\ \\
   \\ %{\tt \small{gabriela.ferraro@nicta.com.au}} \\
}

\date{2014}

% Max 8 pp.

\begin{document}
\maketitle
\begin{abstract}
XXX
\end{abstract}

\newcommand{\gabi}[1]{\textcolor{blue}{#1}}
\newcommand{\tim}[1]{\textcolor{red}{#1}}
\newcommand{\lizhen}[1]{\textcolor{green}{#1}}

\section{Introduction}
Word embedding learning methods are the new generation of distributional
semantics models.  As with other learning methods, it is well known that
the performance of machine learning algorithms heavily depends on
parameter optimization, the training data used and the applications they
target.
% the conextion between dist. semantics models and sequence taggin. 
In this paper, we perform an extensive evaluation of four word embedding
approaches under fixed experiment conditions, and evaluate them over
different sequence labelling tasks.

Expected contributions:
\begin{itemize}
\item[-] Fair comparison of different word embedding algorithms. This
  includes running different word embeddings algorithms under controlled
  conditions, for example, chose algorithms and data sets that are
  publicly available, apply the same NLP pre-processing to all data
  sets, trained the algorithms with the same data sets, apply the
  learned word vectors to well defined tasks sequence labeling tasks
  such as POS-tagging and Named Entity Recognition.

\item[-] Influence of word embeddings in the sequence tagging tasks with
  semi-supervised settings. We considered the empirical evaluation in a
  semi-supervised setting because we conjecture that the word embeddings
  learned from unlabelled data could save a significant amount of
  labelled data, and thus, alleviate the cost of human annotation.
% something about scalability?

\item[-] Comprehensive evaluation of word embeddings in sequence
  labelling tasks.  Sequence labelling tasks is one of the meta problems
  NLP faced. Previous works have shown that learning word embeddings
  vector features improves the performance of sequence labelling task,
  but not fair and extensive comparison has been done so far. In this
  paper, we evaluated word embedding algorithms in several sequence
  labelling tasks and discuss their benefits, taking into account
  different aspects of sequence labelling problems such as row labelling
  (POS-tagging) and join segmentation and labelling (Named Entity
  Recognition).

\item[-] Multiword expressions (MWE) identification using word
  embeddings.  To the best of our knowledge know, current work only
  applies Brown clustering to identify MWE. The experiments carry out in
  this paper will tell which is the best word embedding algorithm to
  solve for the identification of MWE.
\end{itemize}




\section{Related Work}
In this section, we describe the four word embedding approaches we compare in this paper.

Word embeddings models are a new generation of distributional semantic
model (DSM), where the vectors weights are set to optimally predict the
contexts in which the corresponding word tend to appear.

{\color{blue}Describe each of the following word embeddings approaches}
\begin{itemize}
\item[-] Glove \cite{pennington2014glove}
\item[-] word2vec \cite{Mikolov13}
\item[-] Pre-trained word embeddings \cite{Turian10wordrepresentations}
\item[-] Brown cluster \cite{Brown92class-basedn-gram}
% (http://metaoptimize.com/projects/wordreprs/)
\end{itemize}

The first three methods were chosen because they are recent
state-of-the-art word embedding methods and because their software is
available. The final method (Brown clusters) was selected as a benchmark
word representation which makes use of hard word clusters rather than
a distributed representation.
% Brown cluster is not new. Why we chose it?

\tim{Rather than describing the word representations and learning
  methods here, better to describe research which has made use of
  distributed word representations, e.g.:
  \begin{itemize}
  \item \newcite{collobert2011natural}, \newcite{turian2010word}: results to show that distributed
    word representations benefit POS tagging, chunk parsing, SRL, ...
  \item \newcite{owoputi2013improved} and \newcite{Schneider+:2014}: results to show that Brown
    clusters enhance Twitter POS tagging (also \newcite{turian2010word})
    and MWE identification 
  \item Also: word embeddings enhance dependency parsing
    \cite{Bansal+:2014} but NOT constituency parsing \cite{Andreas:Klein:2014}
  \end{itemize}}



\section{Word Representations}

%The statistics about word occurrences in a corpus is the primary source of information available to all supervised methods for learning word representations.
%Traditionally, words were represented in a vocabulary index, were the is not notion about the similarity between words. 

Inspired by distributional semantics models, distributed word
representation methods represent each word as a continuous vector, where
similar words have a similar vector representation, therefore, capturing
the similarity between words.

Example: 

The resulting vectors can be used as features in many NLP applications and it has been shown that they outperform methods that treats words as atomic units \cite{}.

The estimation of the word vectors can be carry out with different models architectures. In this paper, we evaluate the word embedding learning methods presented in the next section and evaluate their utility in several sequence labelling tasks.
 
\subsection{Word Embedding Learning Algorithms}
\begin{itemize}
\item[-] Glove \cite{pennington2014glove}
\item[-] Skip-gram \cite{Mikolov13}
\item[-] CBOW \cite{Mikolov13}
\item[-] Neural language model \cite{Turian10wordrepresentations}
\item[-] Brown cluster \cite{Brown92class-basedn-gram}
% (http://metaoptimize.com/projects/wordreprs/)
\end{itemize}





\subsection{Experimental Setup}

\subsection{Materials}
It is well known that the choice of a corpora have an important effect in the final accuracy of machine learning algorithms. 
Thus, we select different corpora to learn the word embedding vectors (Table \ref{corpus}).
The main reason of choosing these data set is that they are publicly available. 


\begin{table}[h]
\begin{center}
\begin{small}
\begin{tabular}{lll}
\hline
\textbf{Data set} & \textbf{Size} & \textbf{Words} \\ \hline
UMBC 	& 48.1GB & 3 billions \\
One Billion 	& 4.1GB & 0.8 billions  \\
Latest wikipedia dump & 49.6GB & 3 billions \\
Twiter & &  \\ \hline
\end{tabular}
\end{small}
\label{corpus}
\caption{Corpus used to learn word embeddings}
\end{center}
\end{table}

\subsection{Preprocessing}

In order to make the comparison of different word embedding approaches across different applications, we applied the same preprocessing to the data sets used. 
The preprocessing pipeline consist of a sentence splitter, a tokenizer, a POS-tagger and a lemmatizer. The pipeline is built with the UIMA architecture and the DKPro NLP tools. 


\subsection{Parameters}

The performance of each approach heavily depends on their parameters optimization.
In an ideal machine learning setup, grid search or random search would be applied in order to search for the best hyperparameters, for each approach.
But, that is too time taken. Instead, we look for the shared parameters along the three approaches and vary these parameters deterministically? 
The rest of the parameters (the ones that are unique for each approach) are set to  their optimal reported values.
The parameters that we vary are:

%We consider the empirical evaluation in a semi-supervised setting because we conjecture that the word embeddings learned from unlabeled data could save a significant amount of labeled data.

\begin{small}
\begin{itemize}
\item[-]\textbf{Word vector size}: 25, 50, 100, 200, 300, 600.
\item[-]\textbf{Context window size}: 5, 10, 15.
\end{itemize}
\end{small}

Brown clustering: Number of clusters

% Does the format implies any especial pre-preprocesing?  we have plain text and xml

\section{Sequence Tagging Tasks}
We evaluate different learning approaches of word embeddings in four different sequence tagging tasks: POS tagging, chunking, MWE identification, and name entity recognition. Because we can either update pre-trained word embeddings during training or not, through the evaluation, we want to answer the following questions:
\begin{itemize}
\item How well do different word embeddings perform in all tasks when supervised fine-tuning is \textit{not} performed?
\item How well do different word embeddings perform in all tasks when supervised fine-tuning is performed?
\item How does the size of labeled training data affect the experimental results?
\item How well do the word embeddings perform for unknown words? 
\item How do the key parameters of each word learning algorithms affect the experimental results?
\end{itemize}

For each task, we feed learned embeddings into the graph transformer trained with sentence tag criterion~\cite{turian2010word}. The graph transformer is equivalent to CRF, if we do not update word embeddings. For all tasks, we train Graph transformer with pre-trained word embeddings in the following two settings: 
\begin{itemize}
\item CRF with conventional features.
\item Graph transformer \textit{does not} fine tune embeddings during training.
\item Graph transformer fine tunes the embeddings during training.
\end{itemize}
For each task, we split the data into a training set, validation set, and a test set. The hyper parameters are tuned on the validation set with random search~\cite{bergstra2012random}. To be fair, for each model, we randomly choose 100 hyper parameter combinations and pick up the best one based on its performance on the validation set. Then each model is evaluated in a semi-supervised setting. We start with training models on 10\% of the training data, and evaluate them on the test dataset. Then we incrementally add another 10\% of the training data and evaluate them until all training data is used. We adopt per-word F1 scores as the evaluation metric for all tasks except POS tagging. We keep using per-word accuracy for POS tagging. In order to evaluate model performance on unknown words, we report also the average F1 scores for the words that do not occur in the training set.

In order to assess the influence of the key parameters of each word learning algorithms, we evaluate all embedding based models with varying key parameters on the full training set.\lizhen{Do we evaluate also combinations of the key parameters ,e.g. we measure model performance by incrementally augmenting the size of the training set at the same time as the word vector size.?}

\subsection{POS tagging}
We could choose one of the options.
\subsubsection{Option 1}
Almost the same setting as~\cite{collobert2011natural}, except adding one more test set.

\noindent
Training set: 0-18 of WSJ.

\noindent
Validation set: 19-21 of WSJ.

\noindent
Test set: 22-24 of WSJ, and English Web Treebank. We report model performances on these two test sets respectively.

\noindent
Feature space: the same set as in~\cite{collobert2011natural}

\subsubsection{Option 2}

Use the experimental setting in~\cite{owoputi2013improved} for twitter POS tagging. All word embeddings will be learned from the twitter corpus provided by Scott.

\subsection{Chunking} 
The same setting as~\cite{turian2010word}\\

\noindent
Training set: WSJ train set.

\noindent
Validation set: Randomly sampled 1000 sentences from the train set for development.

\noindent
Test set: CoNLL2000 test set.

\noindent
Feature space: the same set as in~\cite{turian2010word}

\subsection{MWE Identification}
Training set: randomly sampled 500 documents from Nathana€™s corpus.

\noindent
Validation set: randomly sampled 100 documents from Nathana€™s corpus.

\noindent
Test set: remaining 123 documents from Nathana€™s corpus..

\noindent
Feature space: the same set as in~\cite{mwecorpus}

\tim{Worth contacting Nathan as possible co-author?}

\subsection{Named entity recognition}
Training set: CoNLL03 train set.

\noindent
Validation set: CoNLL03 development set.

\noindent
Test set: CoNLL03 test set and MUC7. We report model performances on these two test sets respectively.

\noindent
Feature space: the same set as in~\cite{turian2010word}

\section{Experimental Results and Discussion}

\section*{Acknowledgments}

Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\

%NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.

\bibliographystyle{acl2013}
\bibliography{biblio}

\end{document}

