%
% File acl2013.tex
%
% Contact  navigli@di.uniroma1.it
%%
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}



\title{Evaluation of Word Embeddings for Sequence Labelling Tasks}

\author{A Anonymous 
   \\%NICTA / Locked Bag 8001, \\ Canberra ACT 2601, Australia \\
   \\ %The Australian National University\\
   \\ %University of Canberra \\
  \\ % {\tt \small{@nicta.com.au}} \\
\And
  B Anonymous
   \\%NICTA / Locked Bag 8001, \\ Canberra ACT 2601, Australia \\
   \\%The Australian National University\\ \\
   \\ %{\tt \small{@nicta.com.au}} \\
}

\date{2014}

% Max 8 pp.

\begin{document}
\maketitle
\begin{abstract}
XXX
\end{abstract}

\newcommand{\gabi}[1]{\textcolor{blue}{#1}}
\newcommand{\tim}[1]{\textcolor{red}{#1}}
\newcommand{\lizhen}[1]{\textcolor{green}{#1}}
\newcommand{\nss}[1]{\textcolor{magenta}{#1}}

\section{Introduction}
In the last years, distributed word representations
have been applied to several NLP tasks.
Their attractiveness relies in the ability to learn word representations in an unsupervised way, thus directly providing lexical features from big amounts of unlabelled data 
and, therefore, alleviating the cost of human annotations.
It has been also claimed that word embeddings
have the ability to connect out of vocabulary words to known ones, as well known as vocabulary expansion hypothesis. 
Hence, suggesting that word embeddings are a good resource 
for applications that need to be adapted to a certain domain, 
different from the one the application have been tuned for.
For example,...
Another property attribute to word embeddings is their 
capacity to encouraging common behaviour among related in-vocabulary words.

Something about the architectures

As with other learning methods, it is well known that
the performance of machine learning algorithms heavily depends on
parameter optimization, the size of the training data used and the applications they target.
For example, \cite{turian2010word} shows that the optimal word embedding dimensions are task specific.

Moreover, there are several word embeddings methods, which used different algorithms and resources. 
Some methods involve feedback from the end task when learning (or fine-tuning) the word representations and others do not. 
Learning algorithm that involves fine-tuning requires more
memory and takes more time, meanwhile non-fine tuning methods
are less costly.
Why fine-tuning sounds like a good idea?
Whether it is necessary to perform fine-tuning during training 
will help to understand if it is necessary have task-specific 
word representations or not.

Are word embeddings useful for sequence labeling task 
that contained gaps? like MWE


%6) How exactly should word representations be incorporated into feature-based sequence tagging models? (May have been answered by the Guo et al. paper.) 
%I believe there is always a better way of doing this, empirical methods as in Guo’s paper or deep sequence neural networks are the ways to go. However, all of them treat the word embeddings trained with different methods in the same way. I conjecture that we should treat them differently based on how they are trained.


In this paper, we perform an extensive evaluation of four word embedding approaches under fixed experiment conditions, and evaluate them over different sequence labelling tasks: POS-tagging, chunking, 
NER and MWE (Multi Word Expression Identification), 
within the following aims:
(i) perform a fair comparison of different word embeddings
algorithms. This includes running different word embeddings algorithms under controlled conditions, for example, use the same training set, the same preprocessing, etc.
(ii) measure the influence of word embeddings in sequence labeling tasks in semi-supervised settings (fine-tuning)
(iii) use word embeddings for MWE. To the best of our knowledge, 
word embeddings have not been used for this task before.
(iv) systematically compared the usefulness of word embedding versus
unigram features for sequence tagging.

%(i) are word embeddings always better than unigram features? 
%(ii) which word embeddings approach is the best? 
%(iii) to what extend they word embeddings approaches are task specific?
%(iv) how reliable are in labeling out-of-domain data and?

% the conextion between dist. semantics models and sequence taggin. 
static share hypothesis (in vocabulary words, e.g., individual first names are also rare in the treebank, but tend to cluster together in distributional representations);
embedding structure hypothesis (e.g., group words by definiteness, like each, this, every, few, most, etc.).



\iffalse
Expected contributions: 
\begin{itemize}
\item[-] Fair comparison of different word embedding algorithms. This
  includes running different word embeddings algorithms under controlled
  conditions, for example, chose algorithms and data sets that are
  publicly available, apply the same NLP pre-processing to all data
  sets, trained the algorithms with the same data sets, apply the
  learned word vectors to well defined tasks sequence labeling tasks
  such as POS-tagging and Named Entity Recognition.

\item[-] Influence of word embeddings in the sequence tagging tasks with
  semi-supervised settings. We considered the empirical evaluation in a
  semi-supervised setting because we conjecture that the word embeddings
  learned from unlabelled data could save a significant amount of
  labelled data, and thus, alleviate the cost of human annotation.
% something about scalability?
\nss{See Guo et al., ``Revisiting Embedding Features for Simple Semi-Supervised Learning''
(\url{http://ir.hit.edu.cn/~jguo/papers/emnlp2014-semiemb.pdf}).}

\item[-] Comprehensive evaluation of word embeddings in sequence
  labelling tasks.  Sequence labelling tasks is one of the meta problems
  NLP faced. Previous works have shown that learning word embeddings
  vector features improves the performance of sequence labelling task,
  but not fair and extensive comparison has been done so far. In this
  paper, we evaluated word embedding algorithms in several sequence
  labelling tasks and discuss their benefits, taking into account
  different aspects of sequence labelling problems such as row labelling
  (POS-tagging) and join segmentation and labelling (Named Entity
  Recognition).

\item[-] Multiword expressions (MWE) identification using word
  embeddings.  To the best of our knowledge know, current work only
  applies Brown clustering to identify MWE. The experiments carry out in
  this paper will tell which is the best word embedding algorithm to
  solve for the identification of MWE.
\end{itemize}

\fi


{\color{blue} Describe each of the following word embeddings approaches}
\begin{itemize}
\item[-] Glove \cite{pennington2014glove}
\item[-] word2vec \cite{Mikolov13}
\item[-] Pre-trained word embeddings \cite{Turian10wordrepresentations}
\item[-] Brown cluster \cite{Brown92class-basedn-gram}
% (http://metaoptimize.com/projects/wordreprs/)
\end{itemize}

The first three methods were chosen because they are recent
state-of-the-art word embedding methods and because their software is
available. The final method (Brown clusters) was selected as a benchmark
word representation which makes use of hard word clusters rather than
a distributed representation.


\section{Related Work}
Word embedding learning methods are the new generation of distributional semantics models and have been applied to several 
NLP tasks that we summaries in this section.

\newcite{collobert2011natural} proposed a neuronal network architecture
that learn word embeddings and use them in POS-tagging, chunking, NER and SRL. 
%They induced embeddings from a Wikipedia corpus of 631 millon words, with 50 dimensions, over a window size of 5 grams.
%Other hyperparameters they optimize were: capital feature dimension, number of hidden units, and learning rate = 0.01. 
Without specializing their architecture for the mentioned tasks, they achieve close state-of-the-art performance. After including specialized features (e.g., word suffixes for POS-tagging;  Gazzeters for NER, etc.) and other tricks like cascading and ensambling classifiers, achieve competitive state-of-the-art performance.
% they system is very fast too.
Similarly, \newcite{turian2010word} explored the impact of using word
features learned from cluster-based and word embeddings representations
for NER and chunking. 
%They evaluate Brown clusters, and word embeddings from \newcite{Collobert2008}  and \newcite{Mnih2008}. Words representations were learned using the same data set (a sub-sample of the of the RCV1 corpus), of 37 millions words and a vocabulary size of 269K. They induced embeddings with 25, 50, 100, and 200 dimensions over 5-gram windows and 50 epochs for the algorithm from \cite{collobert2011natural}; 50 and 100 dimensions over 5-grams windows and 70 epochs for the algorithm from \cite{Mnih2008}; and clusters solutions of 100, 320, 3200 clusters. According to their experiments, the best scaling parameter, is 0.1.
They conclude that unsupervised word representation improve NER and chunking, and that combining different word representations can further improve the performance.
%They also found that Brown clusters deals better with rare words, since...
Word representation from Brown clusters have been also shown to enhance
Twitter POS tagging \newcite{owoputi2013improved}.  
%They induced 1000 clusters using a data set of 847 millions of tokens (from 56 million unique tweets). Only units that appeared with a frequency higher than 40 were clustered, which results in a vocabulary size of 216,856.

\newcite{Schneider+:2014} presented a MWE analyser that, among other features, used unsupervised word clusters. 
%Using words that appears at least 20 times in the Yelp Academic Dataset (21 million words), they induced 1000 Brown clusters.
They observed that the clusters were useful for identifying words that usually belong to proper names, which are considered MWE in the data set used. Nevertheless, they mentioned that it is difficult to measure the impact of the word embeddings features, since other features may capture the same information. 
%Further feature engineering should be carry out in order to find out the impact 

Word embeddings have been also used as features for syntactic dependency parsing and constituent parsing. 
\newcite{Bansal+:2014} used word embeddings as features for dependency parsing, which used the syntactic dependency context instead of the linear context in raw text. They found that simple attempts based on discretization of individual word vector dimensions do not improve parsing. Only when performing hierarchical clustering of the continuous word vectors then using features based on the hierarchy, they gain performance. They also pointed out that ensemble of different word embeddings representations improved performance.
%An interesting observation the point out is that with large of window size words tend to topically group, and with small window size words tends to share the same POS-tag.
Within the same aim, \cite{Andreas:Klein:2014} explores the used of word embeddings for constituency parsing and conclude that the
information they provide might be redundant with the one acquire by a syntactic parser trained with a small amount of data. Others that boost the performance when including word embeddings representations for syntactic parsing includes \cite{Koo:2008,Koo:2010,Haffari:2011,Tratz:2011}.

Word embedding have also been applied to other (non-sequential NLP) tasks such as super-sense tagging \cite{Grave:2013}; grammar induction \cite{Spitkovsky:2011} and semantic task such as semantic relatedness, synonymy detection, concept categorization selection preferences and analogy \cite{baroni:2014}

% Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. ACL (pp. 595?603).
% Koo et al., 2008; http://cs.nyu.edu/~dsontag/papers/KooEtAl_emnlp10.pdf Dual Decomposition for Parsing with Non-Projective Head Automata - Dependency parsing.

%Haffari et al., 2011; http://www.aclweb.org/anthology/P11-2125 Ensemble of different dependency parsing models, each model corresponding to a different syntactic/semantic word clustering annotation.

% Supersense tagger (Grave et al., 2013) https://hal.inria.fr/hal-00833288/PDF/final-version.pdf





\section{Word Representations}

%The statistics about word occurrences in a corpus is the primary source of information available to all supervised methods for learning word representations.
%Traditionally, words were represented in a vocabulary index, were the is not notion about the similarity between words. 

Inspired by distributional semantics models, distributed word
representation methods represent each word as a continuous vector, where
similar words have a similar vector representation, therefore, capturing
the similarity between words.

Example: 

The resulting vectors can be used as features in many NLP applications and it has been shown that they outperform methods that treats words as atomic units \cite{}. 
The estimation of the word vectors can be carry out with different models architectures. 
We evaluate four different methods, which are the following: 
 
\subsection{Word Embedding Learning Algorithms}
\begin{itemize}
\item[-] Glove \cite{pennington2014glove}
\item[-] Skip-gram \cite{Mikolov13}
\item[-] CBOW \cite{Mikolov13}
\item[-] Neural language model \cite{Turian10wordrepresentations}
\item[-] Brown cluster \cite{Brown92class-basedn-gram}
% (http://metaoptimize.com/projects/wordreprs/)
\end{itemize}



\subsection{Experimental Setup}

\subsection{Materials}
It is well known that the choice of a corpora have an important effect in the final accuracy of machine learning algorithms. 
Therefore, each word embedding method was trained with the same corpora (Table \ref{corpus}). The main reason of choosing the corpora 
is that they are publicly available. 

\begin{table}[h]
\begin{center}
\begin{small}
\begin{tabular}{lll}
\hline
\textbf{Data set} & \textbf{Size} & \textbf{Words} \\ \hline
UMBC 	& 48.1GB & 3 billions \\
One Billion 	& 4.1GB & 0.8 billions  \\
Latest wikipedia dump & 49.6GB & 3 billions \\
\end{tabular}
\end{small}
\label{corpus}
\caption{Corpus used to learn word embeddings}
\end{center}
\end{table}

\subsection{Preprocessing}

In order to make the comparison of different word embedding approaches across different applications, we applied the same preprocessing to the data sets used. 
The preprocessing pipeline consist of a sentence splitter, a tokenizer, a POS-tagger and a lemmatizer. The pipeline is built with the UIMA architecture and the DKPro NLP tools. 


\subsection{Hyperparameters tuning}

The performance of each approach heavily depends on their parameters optimization.
In order to search for the best hyperparameters, we first
look for the shared parameters across methods and perform a random search 
for each method and each task.
The paramenters that are unique for each approach were set to their optimal 
reported values. The parameters that we vary are:

\begin{small}
\begin{itemize}
\item[-]\textbf{Word vector size}: [25, 50, 100, 200, 400, 800].
\item[-]\textbf{Context window size}: [5, 10, 15].
\item[-]\textbf{Number of clusters}: [250, 500, 1000, 2000, 4000]. 
\end{itemize}
\end{small}


\iffalse
\begin{table}
\begin{small}
\begin{tabular}{llllll}
\hline
\multicolumn{6}{c}{\textbf{Brown}} \\ \hline
      		& \textbf{250}  & \textbf{500} & \textbf{1000} & \textbf{2000} & \textbf{4000} \\ \hline
\textbf{POS-Tagging} & 0.885	&  0.708	 & 0.916 & 0.895	 & 0.934  \\     
\textbf{Chunking}	  & 0.945 & 	0.942 &	0.939 &	0.947 &	0.619 \\
\textbf{NER}	& 0.910	& 0.912 & 0.913 & 0.915 & 0.916 \\
\textbf{MWE}	& 	& 0. & 0. & 0. & 0. \\
\hline \hline

\multicolumn{6}{c}{\textbf{No-updated}} \\ \hline
			& CBOW	& CW	 & GLOVE & SKIP & \\
\textbf{POS-Tagging} & 0.943 & 0.915 & 0.922 & 0.945 & \\
\textbf{Chunking} & 0.938 & 0.643 & 	0.614	 & ?  &\\
\textbf{NER}	 & 0.903 & 0.896 & 0.906 & score=NaN  & \\
\textbf{MWE} &  & & & \\

\hline \hline 
\multicolumn{6}{c}{\textbf{Updated}} \\ \hline
 			& CBOW	 & CW	& GLOVE	 & SKIP	 & \\
\textbf{POS-Tagging}  0.953 & 0.943 &	0.954 &	0.954 &	0.910 & \\
\textbf{Chunking} 0.565 &	0.491 &	0.9412  &	?	0.939 & \\
\textbf{NER}	 score=NaN	& score=NaN	& score=NaN	& score=NaN	& 0.878 & \\
\textbf{MWE} &  & & & \\

\hline \hline 
\end{tabular}
\end{small}
\end{table}
\end{comment}
\fi

		



\section{Sequence Tagging Tasks}
We evaluate different learning approaches of word embeddings in four different sequence tagging tasks: POS tagging, chunking, NER and MWE identification.
For each task, we feed learned embeddings into the graph transformer trained with sentence tag criterion~\cite{turian2010word}. The graph transformer is equivalent to CRF, if we do not update word embeddings. For all tasks, we train Graph transformer with pre-trained word embeddings in the following settings: 

\begin{small}
\begin{itemize}
\item[-] CRF with conventional features.
\item[-] Graph transformer \textit{does not} fine tune embeddings during training.
\item[-] Graph transformer fine tunes the embeddings during training.
\end{itemize}
\end{small}

For each task, we split the data into a training set, validation set, and a test set. The hyper parameters are tuned on the validation set with random search~\cite{bergstra2012random}. To be fair, for each model, we randomly choose 100 hyper parameter combinations and pick up the best one based on its performance on the validation set. Then each model is evaluated in a semi-supervised setting. We start with training models on 10\% of the training data, and evaluate them on the test dataset. Then we incrementally add another 10\% of the training data and evaluate them until all training data is used. We adopt per-word F1 scores as the evaluation metric for all tasks except POS tagging, for which we used per-word accuracy.
In order to evaluate model performance on unknown words, we report also the average F1 scores for the words that do not occur in the training set.

In order to assess the influence of the key parameters of each word learning algorithms, we evaluate all embedding based models with varying key parameters on the full training set.????

We also set up experiments to verify that CRF/graph transformer requires different feature design for different kinds of pre-trained word embeddings. One kind of word embeddings is represented by~\cite{bengio2006neural}. It maximises the word sequence likelihood with a model based on a weighted linear combination of word embedding features. Another kind of word embeddings is skip-gram and its variants, which is based on the dot product of two word embeddings. Therefore, we compare at least two ways of representing context word embedding features for each token:

\begin{small}
\begin{itemize}
\item[i] Concatenate word embeddings of context words within a fixed window as context features; 
\item[ii] Concatenate the result of element-wise multiplication of current token embedding and each context word embedding as context features. 
\end{itemize}
\end{small}



\begin{table*}
\begin{small}
\begin{tabular}{lllp{3cm}ll}
			& Training & Validation & Test & Feature space \\ \hline
\textbf{POS-Tagging} & 0-18 of WSJ & 19-21 of WSJ & 22-24 of WSJ and English Web Treebank & as in~\cite{collobert2011natural} \\
\textbf{Chunking} & WSJ & 1000 sentences WSJ & CoNLL2000 & as~\cite{turian2010word}\\
\textbf{NER} & CoNLL2003 train set & CoNLL2003 dev. set & CoNLL2003 test set and MUC7 & as in~\cite{turian2010word} \\
\textbf{MWE} & 500 documents from & 100 documents from & 123 documents & as in~\cite{mwecorpus}\\
\hline
\end{tabular}
\end{small}
\end{table*}


%\subsection{POS tagging} We could choose one of the options. \subsubsection{Option 1} Almost the same setting as~\cite{collobert2011natural}, except adding one more test set.
% \noindent Training set: 0-18 of WSJ.
% \noindent Validation set: 19-21 of WSJ.
% \noindent Test set: 22-24 of WSJ, and English Web Treebank. We report model performances on these two test sets respectively.
% \noindent Feature space: the same set as in~\cite{collobert2011natural}

% \subsection{Chunking} The same setting as~\cite{turian2010word}\\
% \noindent Training set: WSJ train set.
% \noindent Validation set: Randomly sampled 1000 sentences from the train set for development.
% \noindent Test set: CoNLL2000 test set.
% \noindent Feature space: the same set as in~\cite{turian2010word}

% \subsection{MWE Identification} Training set: randomly sampled 500 documents from Nathana��s corpus. 
% \noindent Validation set: randomly sampled 100 documents from Nathana��s corpus.
% \noindent Test set: remaining 123 documents from Nathana��s corpus..
% \noindent Feature space: the same set as in~\cite{mwecorpus}

%\subsection{Named entity recognition} Training set: CoNLL03 train set.
% \noindent Validation set: CoNLL03 development set.
% \noindent Test set: CoNLL03 test set and MUC7. We report model performances on these two test sets respectively.
% \noindent Feature space: the same set as in~\cite{turian2010word}

\section{Experimental Results and Discussion}



Because we can either update pre-trained word embeddings during training or not, through the evaluation, we want to answer the following questions:
\begin{itemize}
\item How well do different word embeddings perform in all tasks when supervised fine-tuning is \textit{not} performed?
\item How well do different word embeddings perform in all tasks when supervised fine-tuning is performed?
\item How does the size of labeled training data affect the experimental results?
\item How well do the word embeddings perform for unknown words? 
\item How do the key parameters of each word learning algorithms affect the experimental results?
\end{itemize}



\section*{Acknowledgments}

Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\
Anonymised\\

%NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program.

\bibliographystyle{acl2013}
\bibliography{biblio}

\end{document}

