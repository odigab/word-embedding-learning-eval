\section{Sequence Tagging Tasks}
\label{sec:SeqTagging}

We evaluate different word representations in four different sequence tagging tasks: POS tagging, chunking, NER and MWE identification. For each task, we feed learned word representations into a first order linear-chain graph transformer~\cite{collobert2011natural}, and trained them by using the online learning algorithm AdaGrad~\cite{duchi2011adaptive}. If we do not update word representations during training. The graph transformer is equivalent to a linear-chain CRF. For each model taking distributed word representations as word features, we consider two settings: 
\begin{small}
\begin{itemize}
\item[-] Graph transformer \textit{does not} fine tune word representations during training.
\item[-] Graph transformer fine tunes the word representations during training.
\end{itemize}
\end{small}
For each task, we consider also CRF models with conventional features, which use one-hot representation for each unigram.

For each task, we split the target corpus into a training set, validation set, and a test set (see Table \ref{datasplit}). If the corpus already provides fixed splits, we just reuse them. For POS tagging and NER, we also evaluate the models with a out-of-domain corpus, which has similar annotation schema as the respective training corpus.

\begin{table*}
\begin{small}
\begin{tabular}{lllp{3cm}ll}
\hline
			& \textbf{Training} & \textbf{Validation} & \textbf{Test} & \textbf{Feature space} \\ \hline
\textbf{POS-Tagging} & 0-18 of WSJ & 19-21 of WSJ & 22-24 of WSJ and English Web Treebank & as in~\cite{collobert2011natural} \\
\textbf{Chunking} & WSJ & 1000 sentences WSJ & CoNLL2000 & as~\cite{turian2010word}\\
\textbf{NER} & CoNLL2003 train set & CoNLL2003 dev. set & CoNLL2003 test set and MUC7 & as in~\cite{turian2010word} \\
\textbf{MWE} & 500 documents from & 100 documents from & 123 documents & as in~\cite{mwecorpus}\\
\hline
\label{datasplit}
\caption{Datasets and features for each task.}
\end{tabular}
\end{small}
\end{table*}

In order to have a fair and reproducible experimental results, we tuned the hyperparameters with random search~\cite{bergstra2012random}. We randomly sample 50 distinct hyperparameter combinations with the same random seed for the models that do not update word embeddings, and sample 100 distinct hyperparameter sets for the models that update word embeddings, and pick up the best performed one based on its performance on the validation set. Note that, we consider word vector size, context window size, and number of brown clusters also as the hyperparameters of respective models. The range of these hyperparameters are given as follows:

\begin{small}
\begin{itemize}
\item[-]\textbf{Word vector size}: [25, 50, 100, 200].
\item[-]\textbf{Context window size}: [5, 10, 15].
\item[-]\textbf{Number of clusters}: [250, 500, 1000, 2000, 4000]. 
\end{itemize}
\end{small}

Similar to~\cite{turian2010word}, all models are trained on their respective training sets. However, for the models that update word representations, we find that we consistently get under-performed hyperparameters after trying out all hyperparameter combinations, because they have more hyperparameters than the models that do not update word representations. Then for each distributed word representations, we reuse all hyperparameters fixed for the corresponding models that do not update word representations, only tune the hyperparameters of AdaGrad for the word representation layer. This method requires only 32 additional runs for each model and achieves consistently better results than 100 random draws.

The final evaluation is carried out in a semi-supervised setting. We split the training set into 10 partitions at log scale. That means, the second smallest partition will be twice the size of the smallest partition. We created 10 training sets with incremental size by merging these partitions from the smallest one to the largest one, and evaluate them all on the same designated test sets. 

We adopt the most commonly used F1 measures as the evaluation metric for all tasks except POS tagging, for which we use per-word accuracy. In order to evaluate model performance on unknown words, we report also the accuracy for the words that do not occur in the training set.

In addition, we also set up experiments to verify if CRF/graph transformer requires different feature design for different kinds of pre-trained word embeddings. This is achieved by adding a hidden layer between CRF and distributed word representations. For each context word, the hidden layer computes the element-wise multiplication of its embedding with the embedding of the current word embedding, and the representation of current word stays the same. The results of this approach are not plotted because this method leads only to marginal improvement.

%\subsection{POS tagging} We could choose one of the options. \subsubsection{Option 1} Almost the same setting as~\cite{collobert2011natural}, except adding one more test set.
% \noindent Training set: 0-18 of WSJ.
% \noindent Validation set: 19-21 of WSJ.
% \noindent Test set: 22-24 of WSJ, and English Web Treebank. We report model performances on these two test sets respectively.
% \noindent Feature space: the same set as in~\cite{collobert2011natural}

% \subsection{Chunking} The same setting as~\cite{turian2010word}\\
% \noindent Training set: WSJ train set.
% \noindent Validation set: Randomly sampled 1000 sentences from the train set for development.
% \noindent Test set: CoNLL2000 test set.
% \noindent Feature space: the same set as in~\cite{turian2010word}

% \subsection{MWE Identification} Training set: randomly sampled 500 documents from Nathana��s corpus. 
% \noindent Validation set: randomly sampled 100 documents from Nathana��s corpus.
% \noindent Test set: remaining 123 documents from Nathana��s corpus..
% \noindent Feature space: the same set as in~\cite{mwecorpus}

%\subsection{Named entity recognition} Training set: CoNLL03 train set.
% \noindent Validation set: CoNLL03 development set.
% \noindent Test set: CoNLL03 test set and MUC7. We report model performances on these two test sets respectively.
% \noindent Feature space: the same set as in~\cite{turian2010word}